{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jgamero-ibrobotics/trabajo-pic/blob/main/TransferLearning.ipynb?hl=es#scrollTo=Lcm7UK0pK7tM\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargar sets de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/media/josh/MyData2SSD/Databases/cifar-10-batches-py'\n",
    "NUM_TRAIN = 45000\n",
    "MINIBATCH_SIZE = 64\n",
    "transform_imagenet = T.Compose([\n",
    "                T.Resize(224),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            ])\n",
    "\n",
    "transform_cifar = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.491, 0.482, 0.447], [0.247, 0.243, 0.261])\n",
    "            ])\n",
    "\n",
    "# Training set loader\n",
    "cifar10_train = datasets.CIFAR10(DATA_PATH, train=True, download=True,\n",
    "                             transform=transform_imagenet)\n",
    "train_loader = DataLoader(cifar10_train, batch_size=MINIBATCH_SIZE, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "# Validation set loader\n",
    "cifar10_val = datasets.CIFAR10(DATA_PATH, train=True, download=True,\n",
    "                           transform=transform_imagenet)\n",
    "val_loader = DataLoader(cifar10_val, batch_size=MINIBATCH_SIZE, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, len(cifar10_val))))\n",
    "\n",
    "# Testing set loader\n",
    "cifar10_test = datasets.CIFAR10(DATA_PATH, train=False, download=True, \n",
    "                            transform=transform_imagenet)\n",
    "test_loader = DataLoader(cifar10_test, batch_size=MINIBATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(val_loader):\n",
    "    print(i, x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Plane', 'Car', 'Bird', 'Cat', 'Deer','Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "def plot_figure(image):\n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "rnd_sample_idx = np.random.randint(len(test_loader))\n",
    "print(f'La imagen muestreada representa un: {classes[test_loader.dataset[rnd_sample_idx][1]]}')\n",
    "image = test_loader.dataset[rnd_sample_idx][0]\n",
    "image = (image - image.min()) / (image.max() -image.min() )\n",
    "plot_figure(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader): # funcion para calcular la precision del modelo\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    epoch_loss = 0.0\n",
    "    model.eval() # ponemos el modelo en modo evaluacion\n",
    "    model = model.to(device=device) # movemos el modelo al device (cpu o gpu)\n",
    "    with torch.no_grad(): # deshabilitamos el calculo del gradiente (no necesita actualizar pesos)\n",
    "\n",
    "        # xi: datos de entrada, yi: etiquetas\n",
    "        for (xi, yi) in loader: # iteramos sobre los minibatches del loader de validacion o test \n",
    "            xi = xi.to(device=device, dtype = torch.float32) # movemos los datos al device (cpu o gpu) \n",
    "            yi = yi.to(device=device, dtype = torch.long) # movemos los datos al device (cpu o gpu)\n",
    "            scores = model(xi) # mb_size, 10. corre el modelo sobre los datos de entrada\n",
    "            loss =  F.cross_entropy(input=scores, target=yi)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            _, pred = scores.max(dim=1) #pred shape (mb_size). obtenemos la clase con mayor score (prediccion)   \n",
    "            # comparamos las predicciones con las etiquetas y sumamos el numero de predicciones correctas\n",
    "            num_correct += (pred == yi).sum() # pred shape (mb_size), yi shape (mb_size, 1).\n",
    "            # sumamos 1 por cada prediccion correcta\n",
    "            num_total += pred.size(0)\n",
    "\n",
    "\n",
    "        loss_ = epoch_loss / len(loader)\n",
    "        accuracy = float(num_correct)/num_total   \n",
    "\n",
    "        return loss_, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader): # funcion para calcular la precision del modelo\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    model.eval() # ponemos el modelo en modo evaluacion\n",
    "    model = model.to(device=device) # movemos el modelo al device (cpu o gpu)\n",
    "    with torch.no_grad(): # deshabilitamos el calculo del gradiente (no necesita actualizar pesos)\n",
    "\n",
    "        # xi: datos de entrada, yi: etiquetas\n",
    "        for (xi, yi) in loader: # iteramos sobre los minibatches del loader de validacion o test \n",
    "            xi = xi.to(device=device, dtype = torch.float32) # movemos los datos al device (cpu o gpu) \n",
    "            yi = yi.to(device=device, dtype = torch.long) # movemos los datos al device (cpu o gpu)\n",
    "            scores = model(xi) # mb_size, 10. corre el modelo sobre los datos de entrada\n",
    "            _, pred = scores.max(dim=1) #pred shape (mb_size). obtenemos la clase con mayor score (prediccion)   \n",
    "            # comparamos las predicciones con las etiquetas y sumamos el numero de predicciones correctas\n",
    "            num_correct += (pred == yi).sum() # pred shape (mb_size), yi shape (mb_size, 1).\n",
    "            # sumamos 1 por cada prediccion correcta\n",
    "            num_total += pred.size(0)\n",
    "\n",
    "        return float(num_correct)/num_total   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar modelo pre-cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploremos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, w in enumerate(model_resnet18.parameters()):\n",
    "    print(i, w.shape, w.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajustar a nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_aux = nn.Sequential(*list(model_resnet18.children()))\n",
    "model_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aux = nn.Sequential(*list(model_resnet18.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, parameter in enumerate(model_aux.parameters()):\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, parameter in enumerate(model_aux.parameters()):\n",
    "    print(i, parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, epochs=100):\n",
    "#     def train(model, optimiser, scheduler = None, epochs=100):\n",
    "    model = model.to(device=device)\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        epoc_val_loss, epoc_val_accuracy = accuracy(model, val_loader)\n",
    "        train_loss, train_accuracy = accuracy(model, train_loader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_accuracy)      \n",
    "        val_loss_history.append(epoc_val_loss)\n",
    "        val_acc_history.append(epoc_val_accuracy) \n",
    "        for i, (xi, yi) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            xi = xi.to(device=device, dtype=torch.float32)\n",
    "            yi = yi.to(device=device, dtype=torch.long)\n",
    "            scores = model(xi)\n",
    "\n",
    "            loss = F.cross_entropy(input= scores, target=yi)\n",
    "        \n",
    "            optimiser.zero_grad()           \n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # calculamos la precision del modelo sobre el conjunto de entrenamiento\n",
    "            # epoch_train_loss += loss.item() # sumamos el costo de los minibatches\n",
    "            # _, pred = scores.max(dim=1) # obtenemos la clase con mayor score (prediccion)\n",
    "            # total_train += yi.size(0) # sumamos el numero de datos en el minibatch\n",
    "            # correct_train += (pred == yi).sum()# sumamos el numero de predicciones correctas   \n",
    "\n",
    "        # train_loss = epoch_train_loss / len(train_loader)\n",
    "        # train_accuracy = correct_train / total_train\n",
    "\n",
    "\n",
    "#         if epoch%5 == 0:     \n",
    "        print(f'Epoch: {epoch}, costo: {loss.item()}, accuracy (validation): {epoc_val_accuracy}, accuracy (train): {train_accuracy},loss (validation): {epoc_val_loss}, loss (train): {train_loss}')\n",
    "#         scheduler.step()\n",
    "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, epochs=100):\n",
    "#     def train(model, optimiser, scheduler = None, epochs=100):\n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (xi, yi) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            xi = xi.to(device=device, dtype=torch.float32)\n",
    "            yi = yi.to(device=device, dtype=torch.long)\n",
    "            scores = model(xi)\n",
    "\n",
    "            cost = F.cross_entropy(input= scores, target=yi)\n",
    "        \n",
    "            optimiser.zero_grad()           \n",
    "            cost.backward()\n",
    "            optimiser.step()           \n",
    "            \n",
    "        acc = accuracy(model, val_loader)\n",
    "        \n",
    "#         if epoch%5 == 0:     \n",
    "        print(f'Epoch: {epoch}, costo: {cost.item()}, accuracy (validation): {acc},')\n",
    "#         scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = 256 \n",
    "hidden = 256\n",
    "lr = 5e-4\n",
    "epochs = 5\n",
    "# model1 = nn.Sequential(nn.Flatten(),\n",
    "#                        nn.Linear(in_features=32*32*3, out_features=hidden1), nn.ReLU(),\n",
    "#                        nn.Linear(in_features=hidden1, out_features=hidden), nn.ReLU(),\n",
    "#                        nn.Linear(in_features=hidden, out_features=10))\n",
    "\n",
    "model1 = nn.Sequential(model_aux,\n",
    "                       nn.Flatten(), \n",
    "                       nn.Linear(in_features=512, out_features= 10, bias= True))\n",
    "optimiser = torch.optim.Adam(model1.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# train(model1, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history, val_loss_history, train_acc_history, val_acc_history = train(model1, optimiser, epochs)\n",
    "# Gráfica de la evolución de la pérdida y la precisión\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss_history, label='train')\n",
    "plt.plot(val_loss_history, label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_acc_history, label='train')\n",
    "plt.plot(val_acc_history, label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model1, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
